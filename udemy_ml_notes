##################################
   Misc Notes Online Course
##################################

########### PYTHON ###############
### check code     : cmd + i   ###
### run code       : cmd enter ###
### set working dir: Fn F5     ###
##################################

############### R ###################
### check code     : Fn F1        ###
### run code       :              ###
### set working dir: files 'more' ###
#####################################



####### PART 2 - Regression Models #############################

### SLR ###

     y      = b0 + b1*x1          //a straight best line, trend 
e.g. salary = b0 + b1*Experience

y dependend var DV
x independent var IV
b0 constant (where line crosses y)
b1 the coefficient, a unit change (slope)

The line is the modelled data
 - take + or - distance, square them SUM(y-y^)^2

* So once data is prepared, lets use Python, then R, to apply SLR model to it :) 

### MLR ###

* Next Multiple Linear Regression, with multiple dependent data
 - Many records, but Profit column being the depended var
  y      = b0 + b1*x1 + b2*x2 ... b4*D1  //a straight line, but steeper 
  Profit        R&D     Expense
  
                       D1   D2
  State is categorical NY   CA (Dummay vars, but you ever incl only N-1)
                        1   0
                        0   1
                        
 - Then Build your Model, step by step:
 
 * Basically, remove X data garbage: But explain why you can't use all 1000 vars...
 
 1. ALL-IN
-------------------------------------------------
  * Prior knowledge, these are the true predictors
  * Framework dictates

 2. BACKWARD SELECTION
-------------------------------------------------
  1. significance level SL 0.05
  2. Fit the full model, everything
  3. Consider predictor with highest P-Val:
      if P > SL {
        step 4
        } else {
            goto 6
            }
  4. Remove the predictor
  5. Fit model without this var
  6. FIN
  
 3. FORWARD SELECTION
-------------------------------------------------
  1. significance level SL 0.05
  2. Fit the full model, everything, select the one with lowest P-val
  3. Keep this var and fit all possible models with one more predictor in addition to it
  4. Out of all these 2var models, consider the predictor with lowest P-Value
      if P < SL {
         goto step 4
         } else {
             goto FIN with the previous model
             }
  5. FIN
  
 4. BIDEIRECTIONAL ELIMINATION
-------------------------------------------------
  1. significance level to enter SLENTER 0.05
     significance level to stay  SLSTAY 0.05
  2. Next step of Forward Selection
     New vars must have P < SLENTER
  3. Perform all steps of Backward Elimination
     Old vars must have P < SLSTAY
  4. No new vars can enter, no old vars can exit
  5. FIN
 
 
5. SCORE COMP (meh, exponentially)
-------------------------------------------------
  1. Compare all possible models
     2^N - 1, so 10 cols(x) 1023 possibilities
 
 
 
                        
  
 - Note of caution: ANY LR makes these assumptions:
     1 Linearity
     2 Homoscedasticity
     3 Multivariate Normality
     4 Independence of errors
     5 Lack of multicollinearity
 
 
####### PART 1 - Data Preprocessing #############################

--- PART 1 - Data preprocessing ---
* In the dataset, find out the independent and dependent cols 
* When running file in python anaconda console: 
  <highlight then control + enter> 
* In R we dont need

* Based on the dataset, lets set to X the MATRIX OF FEATURES, 
* to y the DEPENDEND DATA
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 3].values

* Rows are OBSERVATIONS
* Columns are the FEATURES, and DEPENDEND DATA

* Missing data: use mean

* PenUltimateluy transform relevant cols into Categorical Data
  - So to have only mathematical data in equations, not strings
  - Dummy vars (mask) so to avoid that anything is greater than the other s
  - Now same in R..it's simpler
  
* Eventually, split your data set into the actual training set, and test set
  - This is absolutely required for any ML to see it's working correctly
  - Effectively declare a subset to be the test set, but how to select:
  - Most importantly avoid overfitting (0.2 for Test is usually cool)
  
* Feature Scaling
  - Here: Age and Salary is not on a scale 
  - Euclidian Distance (square of x(age),y(salary)) not at all same scale
  - so standardisation and normalisation necessary > no domination
  - Also, makes ML much faster
  
--- PART 0 - Requirements ---

R Studio
Python 3.5 + Anaconda IDE (numPy, Pandas..)